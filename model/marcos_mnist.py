# -*- coding: utf-8 -*-
"""Marcos MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p_D8UIrzFexHKyrzWeMoEyANEc4ZsAzi
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import numpy as np
from random import shuffle, seed
seed(42)
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (10,10)

from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Input
from keras.utils import np_utils
from keras import optimizers

from keras.layers.core import Flatten
from keras.layers import Conv2D, MaxPooling2D

mnist_classes = 10

# the data, shuffled and split between train and test sets
(X_train, y_train), (X_test, y_test) = mnist.load_data()
print("X_train original shape", X_train.shape)
print("y_train original shape", y_train.shape)

print("\nX_test original shape", X_test.shape)
print("y_test original shape", y_test.shape)

# 80% para treino e 20% para validação
nData = X_train.shape[0]
nTrain = int(nData * 0.8)

randomIdx = list(range(nData))   #randomly select indexes
shuffle(randomIdx)
trainIdx = randomIdx[:nTrain]
valIdx = randomIdx[nTrain:]

# Split the data
X_val, y_val = X_train[valIdx], y_train[valIdx]
X_train, y_train = X_train[trainIdx], y_train[trainIdx]

print("X_train shape", X_train.shape)
print("y_train shape", y_train.shape)

print("X_val shape", X_val.shape)
print("y_val shape", y_val.shape)

for i in range(9):
    plt.subplot(3,3,i+1)
    plt.imshow(X_train[i], cmap='gray', interpolation='none')
    plt.title("Class {}".format(y_train[i]))

# reshape images
X_train = X_train.reshape(48000, 784)
X_val = X_val.reshape(12000, 784)
X_test = X_test.reshape(10000, 784)

X_train = X_train.astype('float32')
X_val = X_val.astype('float32')
X_test = X_test.astype('float32')

X_train = X_train / 255
X_val = X_val / 255
X_test = X_test / 255

print("X_train matrix shape", X_train.shape)
print("Validation matrix shape", X_val.shape)
print("X_test matrix shape", X_test.shape)

Y_train = np_utils.to_categorical(y_train, mnist_classes)
Y_val = np_utils.to_categorical(y_val, mnist_classes)
Y_test = np_utils.to_categorical(y_test, mnist_classes)

print("\ny_train matrix shape", Y_train.shape)
print("y_train matrix shape", Y_val.shape)
print("y_test matrix shape", Y_test.shape)

"""Model 1: This model use a common neural network:"""

model = Sequential()
model.add(Dense(512, input_shape=(784,)))
model.add(Activation('relu')) # An "activation" is just a non-linear function applied to the output
                              # of the layer above. Here, with a "rectified linear unit",
                              # we clamp all values below 0 to 0.

model.add(Dropout(0.2))   # Dropout helps protect the model from memorizing or "overfitting" the training data
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(10))
model.add(Activation('softmax')) # This special "softmax" activation among other things,
                                 # ensures the output is a valid probaility distribution, that is
                                 # that its values are all non-negative and sum to 1.

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train, Y_train,
          batch_size=128, epochs=8, verbose=1,
          validation_data=(X_val, Y_val))

score = model.evaluate(X_test, Y_test, verbose=1)
print('Test score:', score[0])
print('Test accuracy:', score[1])

import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)